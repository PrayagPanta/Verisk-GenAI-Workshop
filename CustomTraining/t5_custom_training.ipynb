{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjqZP72lkyYt5Tn0uZTHI4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "from datasets import load_dataset, load_metric"
      ],
      "metadata": {
        "id": "x9SBN7TYVqav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wi_tGo0dVgAb"
      },
      "outputs": [],
      "source": [
        "# Step 1: Load and Prepare Dataset\n",
        "# Example using a custom translation dataset (replace with your dataset loading logic)\n",
        "# Ensure your dataset is structured with \"source_text\" and \"target_text\" columns\n",
        "dataset = load_dataset(\"csv\", data_files={\"train\": \"train.csv\", \"validation\": \"validation.csv\"})\n",
        "\n",
        "# Preprocess dataset for T5 training\n",
        "def preprocess_function(examples):\n",
        "    # Tokenize the inputs and targets\n",
        "    inputs = tokenizer(examples[\"source_text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "    targets = tokenizer(examples[\"target_text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "    # Update the examples with the tokenized inputs and targets\n",
        "    examples[\"input_ids\"] = inputs.input_ids\n",
        "    examples[\"attention_mask\"] = inputs.attention_mask\n",
        "    examples[\"labels\"] = targets.input_ids\n",
        "\n",
        "    return examples\n",
        "\n",
        "train_dataset = dataset[\"train\"].map(preprocess_function, batched=True)\n",
        "valid_dataset = dataset[\"validation\"].map(preprocess_function, batched=True)\n",
        "\n",
        "# Step 2: Load Tokenizer and Model\n",
        "model_name = 't5-small'\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Step 3: Configure Training Arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    predict_with_generate=True,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    learning_rate=1e-4,\n",
        "    num_train_epochs=3,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "# Step 4: Define Trainer and Train the Model\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=load_metric(\"sacrebleu\"),\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Step 5: Save the Trained Model\n",
        "model.save_pretrained('./trained_t5_translation_model')\n",
        "tokenizer.save_pretrained('./trained_t5_translation_model')"
      ]
    }
  ]
}