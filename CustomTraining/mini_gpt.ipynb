{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOqj9cL50Ng+EYRbHzUr8Su",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samir41939/Verisk-GenAI-Workshop/blob/main/mini_gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmEM8dH9ncvd",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/karpathy/minGPT.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd minGPT"
      ],
      "metadata": {
        "id": "u9RZARWCqxQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e ."
      ],
      "metadata": {
        "id": "QAVqw250rHfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt ."
      ],
      "metadata": {
        "id": "JA67aY3Aur9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "from mingpt.model import GPT\n",
        "from mingpt.trainer import Trainer\n",
        "from mingpt.utils import set_seed, setup_logging, CfgNode as CN"
      ],
      "metadata": {
        "id": "f0X32aJiw2js"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CharDataset(Dataset):\n",
        "  \"\"\"\n",
        "  Emits batches of characters\n",
        "  \"\"\"\n",
        "\n",
        "  @staticmethod\n",
        "  def get_default_config():\n",
        "      C = CN()\n",
        "      C.block_size = 128\n",
        "      return C\n",
        "\n",
        "  def __init__(self, config, data):\n",
        "      self.config = config\n",
        "\n",
        "      chars = sorted(list(set(data)))\n",
        "      data_size, vocab_size = len(data), len(chars)\n",
        "      print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
        "\n",
        "      self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "      self.itos = { i:ch for i,ch in enumerate(chars) }\n",
        "      self.vocab_size = vocab_size\n",
        "      self.data = data\n",
        "\n",
        "  def get_vocab_size(self):\n",
        "      return self.vocab_size\n",
        "\n",
        "  def get_block_size(self):\n",
        "      return self.config.block_size\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.data) - self.config.block_size\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      # grab a chunk of (block_size + 1) characters from the data\n",
        "      chunk = self.data[idx:idx + self.config.block_size + 1]\n",
        "      # encode every character to an integer\n",
        "      dix = [self.stoi[s] for s in chunk]\n",
        "      # return as tensors\n",
        "      x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "      y = torch.tensor(dix[1:], dtype=torch.long)\n",
        "      return x, y"
      ],
      "metadata": {
        "id": "yMh0V6Vnw0Ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_config():\n",
        "  \"\"\"Config setup\"\"\"\n",
        "\n",
        "  C = CN()\n",
        "\n",
        "  # system\n",
        "  C.system = CN()\n",
        "  C.system.seed = 3407\n",
        "  C.system.work_dir = './out/chargpt'\n",
        "\n",
        "  # data\n",
        "  C.data = CharDataset.get_default_config()\n",
        "\n",
        "  # model\n",
        "  C.model = GPT.get_default_config()\n",
        "  C.model.model_type = 'gpt-mini'\n",
        "\n",
        "  # trainer\n",
        "  C.trainer = Trainer.get_default_config()\n",
        "  C.trainer.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\n",
        "\n",
        "  return C"
      ],
      "metadata": {
        "id": "se2DoLQNyIjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = get_config()\n",
        "setup_logging(config)\n",
        "set_seed(config.system.seed)"
      ],
      "metadata": {
        "id": "24Zclx9orJVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# construct the training dataset\n",
        "text = open('input.txt', 'r').read()\n",
        "train_dataset = CharDataset(config.data, text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnSYgNDGyvos",
        "outputId": "13d843f5-40a7-4223-b2ca-01d1fe03d859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 1115394 characters, 65 unique.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# construct the model\n",
        "config.model.vocab_size = train_dataset.get_vocab_size()\n",
        "config.model.block_size = train_dataset.get_block_size()\n",
        "config.model.num_workers = 2\n",
        "model = GPT(config.model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17jVjov6zGNc",
        "outputId": "b1abb229-0835-4974-9a40-1a61c4906e2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 2.71M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# construct the trainer object\n",
        "trainer = Trainer(config.trainer, model, train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsdzKLB4zQqd",
        "outputId": "3062c341-a8c1-449b-8433-7e1ac87215c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running on device cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # iteration callback\n",
        "# def batch_end_callback(trainer):\n",
        "\n",
        "#     if trainer.iter_num % 10 == 0:\n",
        "#         print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
        "\n",
        "#     if trainer.iter_num % 500 == 0:\n",
        "#         # evaluate both the train and test score\n",
        "#         model.eval()\n",
        "#         with torch.no_grad():\n",
        "#             # sample from the model...\n",
        "#             context = \"O God, O God!\"\n",
        "#             x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
        "#             y = model.generate(x, 500, temperature=1.0, do_sample=True, top_k=10)[0]\n",
        "#             completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
        "#             print(completion)\n",
        "#         # save the latest model\n",
        "#         print(\"saving model\")\n",
        "#         ckpt_path = os.path.join(config.system.work_dir, \"model.pt\")\n",
        "#         torch.save(model.state_dict(), ckpt_path)\n",
        "#         # revert model to training mode\n",
        "#         model.train()\n",
        "# trainer.set_callback('on_batch_end', batch_end_callback)"
      ],
      "metadata": {
        "id": "IOOvI3pVzXqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "trainer.run()"
      ],
      "metadata": {
        "id": "8gkBom2a1lFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model inferencing\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "cRMrouKG41Fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  # sample from the model...\n",
        "  context = \"O God, O God!\"\n",
        "  x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
        "  y = model.generate(x, 500, temperature=1.0, do_sample=True, top_k=10)[0]\n",
        "  completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
        "  print(completion)"
      ],
      "metadata": {
        "id": "2_cebSCW5YrM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}