{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMvAV7ru1sBLHjlXOnW03s7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Change Type of Runtime to use GPU.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fXETzumj_Iid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1)**Install required Dependencies**"
      ],
      "metadata": {
        "id": "L_xMhERUMta8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiUb-2oVnu_N"
      },
      "outputs": [],
      "source": [
        "!pip install -i https://pypi.org/simple/ bitsandbytes==0.43.1\n",
        "!pip install accelerate==0.32.1\n",
        "!pip install transformers==4.42.3\n",
        "!pip install torch==2.3.1\n",
        "!pip install langchain==0.2.6\n",
        "!pip install huggingface_hub==0.23.4\n",
        "!pip install langchain_huggingface==0.0.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "id": "8Iv2eAKHG0IB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import and download model Phi-4-mini-4k:**"
      ],
      "metadata": {
        "id": "PafpBQHv_Qb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import transformers\n",
        "\n",
        "#model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model= \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    # The quantization line\n",
        "    model_kwargs={\"torch_dtype\": torch.bfloat16, \"load_in_4bit\": True},\n",
        "    max_new_tokens=1024  # Increase the max tokens to your desired value\n",
        ")"
      ],
      "metadata": {
        "id": "z6vRtBxyNqtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Pipeline**"
      ],
      "metadata": {
        "id": "7IWUKDI1--PP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface.llms import HuggingFacePipeline\n",
        "hf = HuggingFacePipeline(pipeline=pipeline)"
      ],
      "metadata": {
        "id": "QAszHidIahIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"<|user|>\n",
        "Question: {question}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "chain = prompt | hf\n",
        "\n",
        "question = \"What is GenAI?\"\n",
        "\n",
        "print(chain.invoke({\"question\": question}))"
      ],
      "metadata": {
        "id": "ldbQ8U4CeQEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Effective Prompting Strategies:"
      ],
      "metadata": {
        "id": "i9a15MjYOnTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clarifying your Requirements**\n",
        "\n",
        "\n",
        "\n",
        "*   Conciseness\n",
        "*   Complexity\n",
        "*   Format\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kL-4Z4X_O1Cb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5DRliVFgeOMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Hi there, I need something really funny, like a joke or a funny story, but make sure it's about animals because farm animals are hilarious. Thank you so much!\"\n",
        "print(chain.invoke({\"question\": question}))"
      ],
      "metadata": {
        "id": "6Yk9Zr7ADRwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimized Prompt:"
      ],
      "metadata": {
        "id": "hUB7aGMm_qsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimized_question = \"\"\n",
        "print(chain.invoke({\"question\": optimized_question}))"
      ],
      "metadata": {
        "id": "BjO7b398J9bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Using Personas**\n",
        "\n",
        "\n",
        "\n",
        "*   Adopt a particular persona to tailor the tone and style of the response\n",
        "\n"
      ],
      "metadata": {
        "id": "HsLEzjtIEErt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MTcMt9VCEIqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Explain gravity as if you are a pirate. Please restrict the output to about 100 words.\"\n",
        "print(chain.invoke({\"question\": question}))"
      ],
      "metadata": {
        "id": "HpZiRndVEQT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Use Delimiters for Distinct Input Parts**\n",
        "*   Break down input into parts\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "InHOequ1JW9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"Quick advice needed:\n",
        "\n",
        "• Issue: Forgot anniversary\n",
        "• Options: Flowers, jewelry, trip\n",
        "• Best choice: ?\n",
        "\n",
        "Help!\"\"\"\n",
        "print(chain.invoke({\"question\": question}))\n"
      ],
      "metadata": {
        "id": "E3SkKfiYJcde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Including Details in Queries**\n",
        "\n",
        "*   More details you include , more relevant the model's answer is.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T9NoMbvnK5YH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Workout advice\"\n",
        "print(chain.invoke({\"question\": question}))"
      ],
      "metadata": {
        "id": "JAUYyIKHK8fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimized Prompt:"
      ],
      "metadata": {
        "id": "1LoJtmRkAKRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\n",
        "print(chain.invoke({\"question\": question}))"
      ],
      "metadata": {
        "id": "-_zpnyNBLRMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**One Shot Inference**\n",
        "\n",
        "\n",
        "*   Provide one example of prompt-completion pair in the context window to get desired output.\n"
      ],
      "metadata": {
        "id": "5ehv-fpQMo_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = '''Generate a funny joke. Here is an example:\n",
        "Example: Q: Why don't scientists trust atoms? A: Because they make up everything!\n",
        "Now, generate another funny joke.'''\n",
        "\n",
        "print(chain.invoke({\"question\": question}))"
      ],
      "metadata": {
        "id": "6bwOLwMfMrL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Few Shot Inference**\n",
        "\n",
        "\n",
        "*     Provide multiple examples of prompt-completion pair in the context window to get desired output.\n",
        "\n"
      ],
      "metadata": {
        "id": "DYDrMw97NOr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Go through the below examples and generate response for the prompt at the end in similar manner.:\n",
        "**Prompt:**\n",
        "“Trying to impress your date with a romantic dinner? Here are a few key ingredients:\n",
        "\n",
        "1. Candlelight ambiance.\n",
        "2. Homemade pasta (bonus points if you make it yourself!).\n",
        "3. A playlist that says \"I'm suave but not trying too hard.\"\n",
        "\n",
        "**Generated Output:**\n",
        "Alright, here's the recipe for a perfect date night: First, dim the lights until you can barely see your own cooking skills. Then, whip up some noodles like you're auditioning for an Italian grandma role. And lastly, cue up the tunes that scream \"I'm smooth like butter but won't slip off your plate.\" Now go knock 'em dead (figuratively)!\n",
        "\n",
        "**Prompt:**\n",
        "“Planning a cozy movie night at home? Here are the essentials:\n",
        "\n",
        "1. A comfy blanket fort.\n",
        "2. A selection of classic movies.\n",
        "3. Popcorn with a dash of creativity.\n",
        "\n",
        "**Generated Output:**\n",
        "Get ready for the ultimate movie night experience! First, channel your inner architect and build a blanket fort that's as snug as a bug in a rug. Next, line up those timeless films that everyone loves. And finally, don’t just make popcorn – sprinkle some magic with your favorite seasonings. Now, sit back and let the movie magic begin!”\n",
        "\n",
        "**Prompt:**\n",
        "\"Organizing a fun game night with friends? Here's what you'll need:\"\n",
        "\n",
        "<|user|>Please create a list for this prompt as well <|user|>\n",
        "\n",
        "**Generated Output:**\n",
        "<|user|>Please generate output in similar manner to above prompts<|user|>\n",
        "\"\"\"\n",
        "\n",
        "print(chain.invoke({\"question\": prompt}))\n"
      ],
      "metadata": {
        "id": "FjNEW7JfPbjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ys-KXJLSKCcK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kh356O1vWSG8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
