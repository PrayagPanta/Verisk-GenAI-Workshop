{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Let's use Google's T5 model for multiple NLP tasks\n",
        "\n",
        "Documentation: https://pytorch.org/text/0.15.0/tutorials/t5_demo.html"
      ],
      "metadata": {
        "id": "AtEdqswg5Cza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Installing necessary libraries"
      ],
      "metadata": {
        "id": "dppRdElK5qVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers sentencepiece"
      ],
      "metadata": {
        "id": "3hm0USUz6FcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Importing necessary modules"
      ],
      "metadata": {
        "id": "W-sgt5666QwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, pipeline"
      ],
      "metadata": {
        "id": "ckYCXKuq6iIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Load pre-trained T5 model and tokenizer"
      ],
      "metadata": {
        "id": "OehIRNaa-ncz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 't5-small'\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "hh7JlmHxyJhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Using the model for various NLP tasks"
      ],
      "metadata": {
        "id": "NGi_6WZb-zTl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Sentiment Analysis üôÇ**"
      ],
      "metadata": {
        "id": "kb84SNpm_GIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"See, tokenization is fascinating.\"\n",
        "# text = f\"sst2 sentence: {text} </s>\""
      ],
      "metadata": {
        "id": "-u9TiMF4Xk3T"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "print(input_ids)"
      ],
      "metadata": {
        "id": "4z9-2fvoRkIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "_8s5R7UQR2EA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.decode(input_ids[0])\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "5_qTn1lXWdo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_ids = model.generate(input_ids)\n",
        "print(output_ids)"
      ],
      "metadata": {
        "id": "LCYkRM46ZHWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(sentiment)"
      ],
      "metadata": {
        "id": "RDQqQYE5Zu18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get input embeddings\n",
        "input_embeddings = model.get_input_embeddings()(input_ids)\n",
        "print(input_embeddings.shape, input_embeddings)"
      ],
      "metadata": {
        "id": "xVTMuFlGaDiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for sentiment analysis\n",
        "def analyze_sentiment(text):\n",
        "    # The T5 model was trained on the SST2 dataset (also available in torchtext) for sentiment classification using the prefix ‚Äússt2 sentence‚Äù.\n",
        "    # Therefore, we will use this prefix to perform sentiment classification.\n",
        "    input_text = f\"sst2 sentence: {text} </s>\"\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "    # Generate the sentiment classification\n",
        "    output = model.generate(input_ids)\n",
        "    sentiment = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    return sentiment"
      ],
      "metadata": {
        "id": "uhJwPGaYt7Bc"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "text = \"I love this product!\"\n",
        "sentiment = analyze_sentiment(text)\n",
        "print(f\"Sentiment: {sentiment}\")"
      ],
      "metadata": {
        "id": "_BFPMR-wRdLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Text Summarization**"
      ],
      "metadata": {
        "id": "isG5Ee1J_dy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function for text summarization\n",
        "def summarize_text(text, max_length=150):\n",
        "    input_text = f\"summarize: {text} </s>\"\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "    # Generate the summary\n",
        "    summary_ids = model.generate(input_ids, max_length=max_length, early_stopping=True)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "id": "NiLRfGeJuF-A"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "text = \"\"\"The Apollo program was a NASA program that succeeded in landing the first humans on the Moon in 1969.\n",
        "It was started by President John F. Kennedy in 1961 with the goal of landing a man on the Moon and\n",
        "bringing him safely back to Earth before the end of the decade. The program involved a series of manned\n",
        "spaceflights using the Saturn V rocket and the Apollo spacecraft. The first successful manned mission\n",
        "to the Moon was Apollo 11 in July 1969, with astronauts Neil Armstrong, Buzz Aldrin, and Michael Collins.\n",
        "Armstrong and Aldrin became the first humans to walk on the lunar surface, while Collins remained in orbit\n",
        "around the Moon. The Apollo program continued with several successful missions, including scientific\n",
        "exploration and the collection of lunar samples. The last manned mission to the Moon was Apollo 17 in\n",
        "December 1972. The program significantly advanced space exploration and contributed to scientific\n",
        "understanding of the Moon and the broader universe.\"\"\"\n",
        "summary = summarize_text(text)\n",
        "print(f\"Summary: {summary}\")"
      ],
      "metadata": {
        "id": "8z2eHZr2uovl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Language Translation**\n",
        "(Only English to German)"
      ],
      "metadata": {
        "id": "7eUI6l_S_nfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function for language translation\n",
        "def translate_text(text, source_lang='en', target_lang='de'):\n",
        "    # Format inputs as required by T5\n",
        "    input_text = f\"translate {source_lang} to {target_lang}: {text} </s>\"\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "    # Generate the translation\n",
        "    translation_ids = model.generate(input_ids, early_stopping=True)\n",
        "    translation = tokenizer.decode(translation_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return translation"
      ],
      "metadata": {
        "id": "OIz-62ufusck"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "text = \"Hello, how are you?\"\n",
        "translation = translate_text(text, source_lang='English', target_lang='German')\n",
        "print(f\"Translation: {translation}\")"
      ],
      "metadata": {
        "id": "IeS9n10SvPFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Question Answering**"
      ],
      "metadata": {
        "id": "ncYeYbuRAJYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function for question answering\n",
        "def question_answering(context, question):\n",
        "\n",
        "    # Format inputs as required by T5 (prefix context: question:)\n",
        "    input_text = f\"context: {context} question: {question}\"\n",
        "\n",
        "    # Tokenize inputs\n",
        "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate answer\n",
        "    answer_ids = model.generate(inputs, early_stopping=True)\n",
        "    generated_answer = tokenizer.decode(answer_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return generated_answer\n",
        ""
      ],
      "metadata": {
        "id": "boaNwC6IAUw5"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example input for question answering\n",
        "context = \"The Taj Mahal is a famous monument in India. It was built by Emperor Shah Jahan in memory of his wife Mumtaz Mahal.\"\n",
        "question = \"Who built the Taj Mahal?\"\n",
        "\n",
        "# Generate answer\n",
        "generated_answer = question_answering(context, question)\n",
        "\n",
        "print(\"Generated Answer:\", generated_answer)"
      ],
      "metadata": {
        "id": "aqpyU-P_RvDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try a bigger context\n",
        "\n",
        "context2 = \"\"\"\n",
        "The Apollo program was a NASA program that succeeded in landing the first humans on the Moon in 1969.\n",
        "It was started by President John F. Kennedy in 1961 with the goal of landing a man on the Moon and\n",
        "bringing him safely back to Earth before the end of the decade. The program involved a series of manned\n",
        "spaceflights using the Saturn V rocket and the Apollo spacecraft. The first successful manned mission\n",
        "to the Moon was Apollo 11 in July 1969, with astronauts Neil Armstrong, Buzz Aldrin, and Michael Collins.\n",
        "Armstrong and Aldrin became the first humans to walk on the lunar surface, while Collins remained in orbit\n",
        "around the Moon. The Apollo program continued with several successful missions, including scientific\n",
        "exploration and the collection of lunar samples. The last manned mission to the Moon was Apollo 17 in\n",
        "December 1972. The program significantly advanced space exploration and contributed to scientific\n",
        "understanding of the Moon and the broader universe.\n",
        "\"\"\"\n",
        "\n",
        "question2 = \"Who was the first person to walk on the Moon?\"\n",
        "\n",
        "# Generate answer\n",
        "generated_answer2 = question_answering(context2, question2)\n",
        "\n",
        "print(\"Generated Answer:\", generated_answer2)"
      ],
      "metadata": {
        "id": "JyyouFayRzal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Text Generation**"
      ],
      "metadata": {
        "id": "OwTxm8JDZfLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load GPT-2 tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')"
      ],
      "metadata": {
        "id": "Vq_oBxNMZ2sh"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to generate text with specified parameters\n",
        "def generate_text(prompt, temperature=1.0, top_k=50, top_p=0.95, max_length=20):\n",
        "    # Tokenize the input prompt\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "\n",
        "    # Generate text using the model with specified parameters\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=input_ids.shape[1] + 1,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "    # Decode the generated text\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Generate logits for the next token for probability calculation\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids)\n",
        "    logits = outputs.logits[:, -1, :]\n",
        "\n",
        "    # Apply temperature scaling\n",
        "    logits = logits / temperature\n",
        "\n",
        "    # Calculate probabilities using softmax\n",
        "    probs = F.softmax(logits, dim=-1).squeeze()\n",
        "\n",
        "    # Get the top 10 tokens and their probabilities\n",
        "    top_probs, top_indices = torch.topk(probs, 10)\n",
        "    top_tokens = tokenizer.convert_ids_to_tokens(top_indices.tolist())\n",
        "\n",
        "    # Print the top 10 tokens and their probabilities\n",
        "    print(f\"Generated Tokens for the prompt '{prompt}':\")\n",
        "    for token, prob in zip(top_tokens, top_probs):\n",
        "        print(f\"Token: {token}, Probability: {prob.item():.4f}\")\n",
        "\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "lI07AzN1ZpXH"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "prompt = \"I took my dog \"\n",
        "print(\"Original Prompt:\", prompt)\n",
        "\n",
        "# Generate text with different temperature settings\n",
        "print(\"\\nTemperature = 0.7:\")\n",
        "print(generate_text(prompt, temperature=0.7, top_k=2, top_p=0.95))\n",
        "\n",
        "print(\"\\nTemperature = 1.0:\")\n",
        "print(generate_text(prompt, temperature=1.0, top_k=3, top_p=0.6))\n",
        "\n",
        "print(\"\\nTemperature = 1.5:\")\n",
        "print(generate_text(prompt, temperature=1.5, top_k=8, top_p=0.5))"
      ],
      "metadata": {
        "id": "ghAMJNncaGLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jbfNO9ITadgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's use the model easier way - 'pipeline' module\n",
        "Reference: https://huggingface.co/docs/transformers/v4.42.0/en/main_classes/pipelines#transformers.pipeline\n",
        "\n",
        "Pipelines are made of:\n",
        "\n",
        "* A tokenizer in charge of mapping raw textual input to token.\n",
        "* A model to make predictions from the inputs.\n",
        "* Some (optional) post processing for enhancing model‚Äôs output."
      ],
      "metadata": {
        "id": "CTdRbc4mFTUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "t-3gRyYNFQ9e"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\"text2text-generation\", model=\"t5-small\")\n",
        "# pipe = pipeline(\"summarization\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "fQIe7ekxFnDF"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "The Apollo program was a NASA program that succeeded in landing the first humans on the Moon in 1969.\n",
        "It was started by President John F. Kennedy in 1961 with the goal of landing a man on the Moon and\n",
        "bringing him safely back to Earth before the end of the decade. The program involved a series of manned\n",
        "spaceflights using the Saturn V rocket and the Apollo spacecraft. The first successful manned mission\n",
        "to the Moon was Apollo 11 in July 1969, with astronauts Neil Armstrong, Buzz Aldrin, and Michael Collins.\n",
        "Armstrong and Aldrin became the first humans to walk on the lunar surface, while Collins remained in orbit\n",
        "around the Moon. The Apollo program continued with several successful missions, including scientific\n",
        "exploration and the collection of lunar samples. The last manned mission to the Moon was Apollo 17 in\n",
        "December 1972. The program significantly advanced space exploration and contributed to scientific\n",
        "understanding of the Moon and the broader universe.\n",
        "\"\"\"\n",
        "input_text_format = f\"summarize: {text} </s>\"\n",
        "summary = pipe(input_text_format, max_length=150)\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWgjCDZoF7TE",
        "outputId": "debb7ee0-d3cd-4661-af59-c7d8979c8b15"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'the Apollo program was started by president . Kennedy in 1961 . it involved a series of manned spaceflights using the Saturn V rocket and the Apollo spacecraft .'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary = pipe(input_text_format, max_length=150, do_sample=True, top_k=10, top_p=0.90, temperature=0.7)\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "E_gvtLM6La61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# speech to text recognition\n",
        "speech_to_text = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base\")"
      ],
      "metadata": {
        "id": "YZkMFIjpLlh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_text = speech_to_text(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac\")\n",
        "print(output_text)"
      ],
      "metadata": {
        "id": "bOVNgAlVXJ8b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}